{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict, namedtuple\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from munkres import Munkres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StemmedDocument = namedtuple(\"StemmedDocument\", [\"name\", \"word_counts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = \"../documents/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filenames(folder):\n",
    "    return sorted([f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))])\n",
    "\n",
    "  \n",
    "def read_article(folder, name):\n",
    "    content = defaultdict(int)\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    stemmer = PorterStemmer()\n",
    "    with open(FOLDER + name, \"r\") as article:\n",
    "        for line in article:\n",
    "            for word in line.split():\n",
    "                word = word.lower()\n",
    "                word = re.sub(\"^[^a-z]*|[^a-z]*$\", \"\", word)\n",
    "                if word and word not in stop_words:\n",
    "                    word = stemmer.stem(word)\n",
    "                    content[word] += 1\n",
    "    return dict(content)\n",
    "\n",
    "\n",
    "def read_documents(folder):\n",
    "    documents = []\n",
    "    filenames = get_filenames(folder)\n",
    "    for fname in filenames:\n",
    "        if not fname.startswith(\"summary\"):\n",
    "            content = read_article(folder, fname)\n",
    "            documents.append(StemmedDocument(fname, content))\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = read_documents(FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document_term_matrix(documents):\n",
    "    vectorizer = DictVectorizer(dtype=int, sparse=True)\n",
    "    count_matrix = vectorizer.fit_transform(map(lambda x: x.word_counts, documents))\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    transformer = TfidfTransformer(norm=\"l2\", sublinear_tf=True)\n",
    "    term_matrix = transformer.fit_transform(count_matrix)\n",
    "    document_titles = list(map(lambda x: x.name, documents))\n",
    "    return term_matrix, document_titles, terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_matrix, titles, terms = create_document_term_matrix(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate topic stability to decide how many topics are there.\n",
    "\n",
    "For each size $k$ reference topics are calculated. Each topic is represented by a list containing the indices of the top 20 words. Then $80\\%$ of the documents are selected and topic representations are recalculated. Now we have two set of the same number of ordered lists having the same number of elements. Then we calculate a distance matrix between the topics in the reference topics and the sampled topics, respectively. Then for each topic in the reference topic we assign a topic in the sample topics in a bijective manner using the Hungarian Method.\n",
    "\n",
    "As an example, imagine that the top $4$ terms in $3$ topics are\n",
    "* tennis, racket, grass, wimbledon\n",
    "* pollution, coal, plant, electricity\n",
    "* electricity, storm, severe, wimbledon\n",
    "\n",
    "and the sampled documents resulted in the following top words:\n",
    "\n",
    "* tennis, semi, racket, crown\n",
    "* pollution, coal, nuclear, plant\n",
    "* storm, weather, electricity, severe\n",
    "\n",
    "The resulting similarity matrix based on average Jaccard-distance is\n",
    "\\begin{matrix} 0.5417 & 0 & 0 \\\\ 0 & 0.775 & 0.0357 \\\\ 0 & 0 & 0.3583 \\end{matrix}\n",
    "\n",
    "This is not a symmetric matrix! The last task is to pair a reference topic with a sample topic such that their total similarities maximized (or the sitance is minimized), which is now $\\frac{0.5417 + 0.775 + 0.3583}{3} = 0.558$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING_RATE = 0.8\n",
    "NUMBER_OF_TRIALS = 40\n",
    "\n",
    "NumberOfTopics = namedtuple(\"NumberOfTopics\", [\"size\", \"stability_score\"])\n",
    "\n",
    "\n",
    "def get_top_terms_from_topics(term_matrix, cluster_number, nr_top_terms):\n",
    "    model = NMF(n_components=cluster_number, init=\"nndsvd\", solver=\"cd\", alpha=0.3, random_state=0).fit(term_matrix)\n",
    "    top_terms_in_each_topic = []\n",
    "    for topic in model.components_:\n",
    "        top_terms = tuple(ix for ix in np.flipud(topic.argsort())[:nr_top_terms])\n",
    "        top_terms_in_each_topic.append(top_terms)\n",
    "    return tuple(top_terms_in_each_topic)    \n",
    "\n",
    "\n",
    "def calc_average_jaccard_measure(reference_topic, sample_topic, nr_top_terms):\n",
    "    measure = 0\n",
    "    for d_value in range(1, nr_top_terms + 1):\n",
    "        top_d_reference_terms = set(reference_topic[:d_value])\n",
    "        top_d_sample_terms = set(sample_topic[:d_value])\n",
    "        intersection_of_top_d_terms = top_d_reference_terms.intersection(top_d_sample_terms)\n",
    "        union_of_top_d_terms = top_d_reference_terms.union(top_d_sample_terms)\n",
    "        measure += len(intersection_of_top_d_terms) / len(union_of_top_d_terms)\n",
    "    return measure / nr_top_terms\n",
    "\n",
    "\n",
    "def calc_similarity_matrix(reference_topics, sample_topics, cluster_number, top_terms):\n",
    "    similarity_matrix = np.zeros((cluster_number, cluster_number))\n",
    "    for ix, reference_topic in enumerate(reference_topics):\n",
    "        for jx, sample_topic in enumerate(sample_topics):\n",
    "            similarity_matrix[ix, jx] = calc_average_jaccard_measure(reference_topic, sample_topic, top_terms)\n",
    "    return similarity_matrix\n",
    "\n",
    "\n",
    "def calc_agreement_score(reference_topics, sample_topics, cluster_number, nr_top_terms):\n",
    "    similarity_matrix = calc_similarity_matrix(reference_topics, sample_topics, cluster_number, nr_top_terms)\n",
    "    HungarianMethod = Munkres()\n",
    "    maximal_agreement_path = HungarianMethod.compute(1 - similarity_matrix)\n",
    "    return sum([similarity_matrix[ix, jx] for ix, jx in maximal_agreement_path]) / cluster_number\n",
    "\n",
    "\n",
    "def calc_stability_score(term_matrix, cluster_number, nr_top_terms):\n",
    "    reference_topics = get_top_terms_from_topics(term_matrix, cluster_number, nr_top_terms)\n",
    "    number_of_documents = term_matrix.shape[0]\n",
    "    np.random.seed(seed=1)\n",
    "    stability_score = 0\n",
    "    for _ in range(NUMBER_OF_TRIALS):\n",
    "        sample_rows = np.random.choice(number_of_documents, int(SAMPLING_RATE * number_of_documents), replace=False)\n",
    "        sample_topics = get_top_terms_from_topics(term_matrix[sample_rows, :], cluster_number, nr_top_terms)\n",
    "        stability_score += calc_agreement_score(reference_topics, sample_topics, cluster_number, nr_top_terms)\n",
    "    return stability_score / NUMBER_OF_TRIALS\n",
    "\n",
    "\n",
    "def estimate_number_of_clusters_by_topic_stability(term_matrix, cluster_number_candidates):\n",
    "    number_of_documents, number_of_terms = term_matrix.shape\n",
    "    maximum_number_of_clusters = min(number_of_terms, int(SAMPLING_RATE * number_of_documents))\n",
    "    cluster_number_candidates = [cluster_number for cluster_number in cluster_number_candidates\n",
    "                                 if cluster_number <= maximum_number_of_clusters]\n",
    "    nr_top_terms = min(20, number_of_terms)\n",
    "    stability_scores = []\n",
    "    for cluster_number in cluster_number_candidates:\n",
    "        stability = calc_stability_score(term_matrix, cluster_number, nr_top_terms)\n",
    "        stability_scores.append(NumberOfTopics(cluster_number, stability))\n",
    "    optimal_cluster_number = max(stability_scores, key=lambda item: item.stability_score)[0]\n",
    "    return optimal_cluster_number, stability_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_number_candidates = range(3, 10)\n",
    "number_of_topics, stabilities = estimate_number_of_clusters_by_topic_stability(term_matrix, cluster_number_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMF(n_components=number_of_topics, init=\"nndsvd\", solver=\"cd\", alpha=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = model.fit_transform(term_matrix)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(matrix, feature_names, n_top_words):\n",
    "    for topic_id, topic in enumerate(matrix):\n",
    "        topic = topic / np.linalg.norm(topic)\n",
    "        print(\"Topic #%d:\" % topic_id)\n",
    "        print(\" | \".join([str(feature_names[ix]) for ix in topic.argsort()[:-n_top_words-1:-1] if topic[ix] > 1e-12]))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top_words(H, terms, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top_words(W.T, titles, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_coverage(matrix, users):\n",
    "    matrix = (matrix.T / matrix.sum(axis=1)).T\n",
    "    number_of_topics = matrix.shape[1]\n",
    "    colnames = [\"Topic #{}\".format(str(ix)) for ix in range(number_of_topics)]\n",
    "    return pd.DataFrame(data=np.round(100 * matrix, decimals=2), columns=colnames, index=users)\n",
    "\n",
    "  \n",
    "def get_user_importance_in_topics(matrix, users):\n",
    "    matrix = matrix / matrix.sum(axis=0)\n",
    "    number_of_users, number_of_topics = matrix.shape\n",
    "    importance = np.zeros((number_of_users, number_of_topics), dtype=np.object)\n",
    "    important_user_indexes = np.flipud(np.argsort(matrix, axis=0))\n",
    "\n",
    "    for ix, user in enumerate(users):\n",
    "        row = [important_user_indexes[:, jx].tolist().index(ix) + 1 if matrix[ix, jx] > 1e-12 else \"\"\n",
    "               for jx in range(number_of_topics)]\n",
    "        importance[ix, :] = np.array(row)\n",
    "\n",
    "    colnames = [\"Topic #{}\".format(str(ix)) for ix in range(number_of_topics)]\n",
    "    return pd.DataFrame(data=importance, columns=colnames, index=users)\n",
    "\n",
    "def get_user_weight_percentages_in_topics(matrix, users):\n",
    "    matrix = matrix / matrix.sum(axis=0)\n",
    "    number_of_users, number_of_topics = matrix.shape\n",
    "    colnames = [\"Topic #{}\".format(str(ix)) for ix in range(number_of_topics)]\n",
    "    return pd.DataFrame(data=np.round(100 * matrix, decimals=4), columns=colnames, index=users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Topic coverage for each document (percents)\n",
    "df_coverage = get_topic_coverage(W, titles)\n",
    "df_coverage.head(n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#document importance in topics\n",
    "#For each document the importance of that document in each topic (that is, how important a document in a given cluster).\n",
    "#If the weight of the document in a topic is zero then we do not assign importance to that document in that topic.\n",
    "df_importance = get_user_importance_in_topics(W, titles)\n",
    "df_importance.head(n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#document weight percentages in topics.\n",
    "df_weight_percents = get_user_weight_percentages_in_topics(W, titles)\n",
    "df_weight_percents.head(n=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
