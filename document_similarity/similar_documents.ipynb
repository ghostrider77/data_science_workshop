{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict, namedtuple\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from datasketch import MinHashLSHForest, MinHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = \"../documents/\"\n",
    "StemmedDocument = namedtuple(\"StemmedDocument\", [\"name\", \"word_counts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filenames(folder):\n",
    "    return sorted([f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))])\n",
    "\n",
    "  \n",
    "def read_article(folder, name):\n",
    "    content = defaultdict(int)\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    stemmer = PorterStemmer()\n",
    "    with open(FOLDER + name, \"r\") as article:\n",
    "        for line in article:\n",
    "            for word in line.split():\n",
    "                word = word.lower()\n",
    "                word = re.sub(\"^[^a-z]*|[^a-z]*$\", \"\", word)\n",
    "                if word and word not in stop_words:\n",
    "                    word = stemmer.stem(word)\n",
    "                    word = word.encode(\"utf-8\")\n",
    "                    content[word] += 1\n",
    "    return dict(content)\n",
    "\n",
    "\n",
    "def read_documents(folder):\n",
    "    documents = []\n",
    "    filenames = get_filenames(folder)\n",
    "    for fname in filenames:\n",
    "        if not fname.startswith(\"summary\"):\n",
    "            content = read_article(folder, fname)\n",
    "            documents.append(StemmedDocument(fname, content))\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = read_documents(FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document_term_matrix(documents):\n",
    "    vectorizer = DictVectorizer(dtype=int, sparse=True)\n",
    "    count_matrix = vectorizer.fit_transform(map(lambda x: x.word_counts, documents))\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    transformer = TfidfTransformer(norm=\"l2\", sublinear_tf=True)\n",
    "    term_matrix = transformer.fit_transform(count_matrix)\n",
    "    document_titles = list(map(lambda x: x.name, documents))\n",
    "    return term_matrix, document_titles, terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_matrix, labels, words = create_document_term_matrix(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = pd.DataFrame((term_matrix * term_matrix.T).A, columns=labels, index=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_k_similar_documents(similarity_matrix, document_name, k):\n",
    "    row = similarity_matrix.loc[document_name, similarity_matrix.columns != document_name]\n",
    "    return row.sort_values(ascending=False)[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_top_k_similar_documents(similarity_matrix, \"uk_5.txt\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSHForest(object):\n",
    "    def __init__(self, nr_permutations):\n",
    "        self._nr_permutations = nr_permutations\n",
    "        \n",
    "    def build_lsh_forest(self, documents):\n",
    "        forest = MinHashLSHForest(num_perm=self._nr_permutations)\n",
    "        for document_name, word_counts in documents:\n",
    "            minhash = MinHash(num_perm=self._nr_permutations)\n",
    "            for word, cnt in word_counts.items():\n",
    "                for _ in range(cnt):\n",
    "                    minhash.update(word)\n",
    "            forest.add(document_name, minhash)\n",
    "        forest.index()\n",
    "        return forest\n",
    "      \n",
    "    def _query_indices_of_most_similar_documents(self, forest, words, k):\n",
    "        minhash = MinHash(num_perm=self._nr_permutations)\n",
    "        for word, count in words.items():\n",
    "            for _ in range(count):\n",
    "                minhash.update(word)\n",
    "        return forest.query(minhash, k)\n",
    "      \n",
    "    def get_top_k_most_similar_documents(self, forest, documents, query_document, k):\n",
    "        words = get_words_of_document(query_document, documents)\n",
    "        top_k_indices = self._query_indices_of_most_similar_documents(forest, words, k)\n",
    "        return top_k_indices\n",
    "\n",
    "\n",
    "def get_words_of_document(query_document, documents):\n",
    "    for name, words in documents:\n",
    "        if name == query_document:\n",
    "            return words\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsh_forest = LSHForest(nr_permutations=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = lsh_forest.build_lsh_forest(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approximate_neighbours = lsh_forest.get_top_k_most_similar_documents(forest, documents, \"uk_5.txt\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approximate_neighbours"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
